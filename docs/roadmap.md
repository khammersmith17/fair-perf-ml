
# Fair-Perf-ML ‚Äî Real-Time Observability Roadmap

## Overview

**Fair-Perf-ML** is a lightweight, composable observability toolkit for machine-learning systems.  
It provides a unified API for computing **performance**, **fairness**, and soon **drift** metrics in both
batch and streaming settings.

The next major phase of the project focuses on **real-time drift detection** and **online monitoring** ‚Äî
building the bridge between offline evaluation and live production observability.

---

## ‚ú® Vision

Enable engineers to detect model degradation, bias, or data drift **before** ground truth arrives.  
Fair-Perf-ML will support real-time metric computation over streaming or micro-batch data, with
plug-and-play integration for existing ML pipelines (SageMaker, Airflow, Kafka, Kinesis, etc.).

---

## üß© Core Goals

1. **Unified metric API** ‚Äî same interface for performance, fairness, and drift metrics.
2. **Real-time drift monitoring** ‚Äî lightweight statistical proxies for early detection.
3. **Streaming-friendly design** ‚Äî incremental metrics, windowed evaluation, low memory.
4. **Fairness-aware drift** ‚Äî group-level monitoring for ethical robustness.
5. **Interoperability** ‚Äî easy export to OLAP, Prometheus, CloudWatch, or S3 JSON logs.

---

## Roadmap

### **Phase 1 ‚Äì Drift Metrics (Q4 2025)**

**Goal:** introduce a first-class `fair_perf_ml.drift` module with standard metrics.

#### Features
- [X] `PopulationStabilityIndex` (PSI) ‚Äî categorical + numeric.
- [X] `KSTestDrift` ‚Äî two-sample Kolmogorov‚ÄìSmirnov test.
- [X] `JensenShannonDrift` ‚Äî symmetric divergence [0, 1].
- [ ] `WassersteinDrift` ‚Äî Earth-Mover‚Äôs distance for continuous features.
- [ ] Unified base class `BaseDriftMetric` with `compute(baseline, current)`.

#### Deliverables
- [ ] Comprehensive unit tests for numeric / categorical inputs.
- [ ] Documentation & usage examples (`drift_examples.ipynb`).
- [ ] Serialization for baseline histograms (`.json`, `.parquet`).

---

### **Phase 2 ‚Äì Real-Time Monitoring Engine (Q1 2026)**

**Goal:** make drift and fairness metrics stream-aware and incremental.

#### Features
- [ ] `DriftMonitor` class for rolling window updates.
- [ ] Online statistics via Welford‚Äôs algorithm / T-Digest summaries.
- [ ] Config-driven thresholds and per-feature policies.
- [ ] Aggregation & alert logic (e.g., drift detected in ‚â• K % of features).
- [ ] Async / background execution hooks.

#### Deliverables
- [ ] Example micro-batch monitor (5-min window) for Kafka/Kinesis.
- [ ] CLI & YAML config interface.
- [ ] Integration guide: ‚ÄúUsing Fair-Perf-ML in a streaming service‚Äù.

---

### **Phase 3 ‚Äì Fairness Drift & Group Analytics (Q2 2026)**

**Goal:** extend drift detection to subgroup and fairness metrics.

#### Features
- [ ] Group-level PSI / JSD (e.g., `PSI(≈∑ | gender)`).
- [ ] Fairness delta tracking (Œî FPR, Œî TPR, Œî PPV over time).
- [ ] Group-specific drift alerts.
- [ ] Fairness-drift visualization templates.

#### Deliverables
- [ ] Example notebook: ‚ÄúMonitoring fairness drift in real time‚Äù.
- [ ] API design doc for `fair_perf_ml.fairness.monitor`.

---

### **Phase 4 ‚Äì Observability Integrations (Q3 2026)**

**Goal:** make Fair-Perf-ML metrics observable and interoperable with production systems.

#### Features
- [ ] Exporters for:
  - Prometheus Pushgateway
  - AWS CloudWatch
  - OpenTelemetry Metrics API
  - S3 / DynamoDB JSON logs
- [ ] `MetricPublisher` abstraction layer.
- [ ] Prebuilt Grafana dashboards for PSI, drift scores, fairness deltas.

#### Deliverables
- [ ] Example Docker sidecar for model endpoints.
- [ ] `fair_perf_ml.client` module with async publishers.
- [ ] Integration tests with SageMaker & local streaming setup.

---

### **Phase 5 ‚Äì Continuous Evaluation & Ground-Truth Integration (Q4 2026)**

**Goal:** combine real-time proxy metrics with delayed ground-truth metrics.

#### Features
- [ ] Join inference logs with delayed labels for backfilled evaluation.
- [ ] Compare leading (proxy) vs lagging (ground-truth) metrics.
- [ ] Automatic model performance dashboards (F1, AUC, Recall drift).
- [ ] Correlation tracking between proxy drift and true performance drop.

---

## üß† Architecture Summary

